{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45f44fb7",
   "metadata": {},
   "source": [
    "# MLflow 3.3 Evaluation API Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c77fc3",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we will import the necessary libraries and configure the OpenAI client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48a98a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "import openai\n",
    "import mlflow\n",
    "from mlflow.genai.scorers import Correctness, Guidelines\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcb0d06",
   "metadata": {},
   "source": [
    "Next, start the tracking server by running:\n",
    "\n",
    "```bash\n",
    "mlflow ui --port 5000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23baee8",
   "metadata": {},
   "source": [
    "Lastly, we will specify the tracking URI and create a new experiment for logging these evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a039d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/653076567649131449', creation_time=1755125128603, experiment_id='653076567649131449', last_update_time=1755125128603, lifecycle_stage='active', name='mlflow-genai-eval', tags={}>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "\n",
    "mlflow.set_experiment(\"mlflow-genai-eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c492c35a",
   "metadata": {},
   "source": [
    "## Define Evaluation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49887fc4",
   "metadata": {},
   "source": [
    "Next, we define a simple evaluation dataset. This dataset only consists of inputs and expected outputs. Note that we can structure our evaluation dataset in a number of different ways. We can even create an evaluation set from previously-generated traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e2a61c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Can MLflow manage prompts?\"},\n",
    "        \"expectations\": {\"expected_response\": \"Yes!\"},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Can MLflow create a taco for my lunch?\"},\n",
    "        \"expectations\": {\n",
    "            \"expected_response\": \"No, unfortunately, MLflow is not a taco maker.\"\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103d775c",
   "metadata": {},
   "source": [
    "## Define Predict Function\n",
    "\n",
    "Next we define our `predict_fn`. This is the function that is used to generate predictions. Note how the argument (`questions`) matches the `inputs` in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c7f233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fn(question: str) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-5-nano\", messages=[{\"role\": \"user\", \"content\": question}]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725b59f9",
   "metadata": {},
   "source": [
    "## Run the Evaluation\n",
    "\n",
    "Lastly, we run the evaluation. We will use the built-in `Correctness` scorer, along with two custom \"Guidelines\" scorers. `Guidelines` make it easy to add custom LLM-based true/false criteria using natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63541948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/14 09:26:00 INFO mlflow.genai.utils.data_validation: Testing model prediction with the first sample in the dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "('Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [Elapsed: 00:19, Remaining: 00:00] ',)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run aged-hen-421 at: http://localhost:5000/#/experiments/653076567649131449/runs/965f93bfcb964b54a53a8dc17521bf0b\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/653076567649131449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mlflow.openai.autolog()\n",
    "\n",
    "results = mlflow.genai.evaluate(\n",
    "    data=dataset,\n",
    "    predict_fn=predict_fn,\n",
    "    scorers=[\n",
    "        # Built-in LLM judge\n",
    "        Correctness(),\n",
    "        # Custom criteria using LLM judge\n",
    "        Guidelines(name=\"is_english\", guidelines=\"The answer must be in English\"),\n",
    "        Guidelines(name=\"is_concise\", guidelines=\"The answer must answer the question concisely, without excessive explanation or followup questions.\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5b95e3",
   "metadata": {},
   "source": [
    "## View results in MLflow UI\n",
    "\n",
    "Now that we have run the evaluation, we can view the results in the UI!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
