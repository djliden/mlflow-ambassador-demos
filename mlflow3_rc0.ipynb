{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow 3 RC0\n",
    "\n",
    "- [Docs](https://mlflow.org/docs/3.0.0rc0/mlflow-3/)\n",
    "- Changing parameters etc changes the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import openai\n",
    "\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5001\")\n",
    "mlflow.set_experiment(\"mlflow3-rc0\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello, world!\"}],\n",
    "        temperature=0.1,\n",
    "        max_tokens=2000,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register a Prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = mlflow.register_prompt(\n",
    "    name=\"summarization-prompt\",\n",
    "    template=\"\"\"Summarize the following text at the provided level of complexity, where 1 represents a non-technical summary \n",
    "for novices and 5 represents a technical summary for experts.\\n\\nLevel: {{level}}\\n\\nText: {{text}}\"\"\",\n",
    "    # Optional: Provide a commit message to describe the changes\n",
    "    commit_message=\"Initial commit\",\n",
    "    # Optional: Specify any additional metadata about the prompt version\n",
    "    version_metadata={\n",
    "        \"author\": \"Daniel Liden\",\n",
    "    },\n",
    "    # Optional: Set tags applies to the prompt (across versions)\n",
    "    tags={\n",
    "        \"task\": \"summarization\",\n",
    "        \"language\": \"en\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke the Model with the Registered Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from markdownify import markdownify\n",
    "# helper function to get text from a webpage\n",
    "\n",
    "def webpage_to_markdown(url):\n",
    "    # Get webpage content\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "    \n",
    "    # Convert to markdown\n",
    "    markdown_content = markdownify(html_content)\n",
    "\n",
    "    return markdown_content\n",
    "\n",
    "url = \"https://mlflow.org/docs/3.0.0rc0/tracing/tracing-schema\"\n",
    "markdown_content = webpage_to_markdown(url)\n",
    "\n",
    "# Invoke the model with the registered prompt\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt.format(level=1, text=markdown_content)}],\n",
    "    temperature=0.1,\n",
    "    max_tokens=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt.format(level=5, text=markdown_content)}],\n",
    "    temperature=0.1,\n",
    "    max_tokens=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.metrics.genai import faithfulness\n",
    "\n",
    "faithfulness_metric = faithfulness(model=\"openai:/gpt-4o\")\n",
    "\n",
    "# Fetch the LoggedModel that's automatically created during autologging\n",
    "logged_model = mlflow.last_logged_model()\n",
    "\n",
    "with mlflow.start_run():\n",
    "    for level in range(1, 6):   \n",
    "        response = (\n",
    "            client.chat.completions.create(\n",
    "                messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt.format(level=level, text=markdown_content)}\n",
    "            ],\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.1,\n",
    "            max_tokens=2000,\n",
    "        )\n",
    "        .choices[0]\n",
    "        .message.content\n",
    "    )\n",
    "\n",
    "        faithfulness_score = faithfulness_metric(\n",
    "            predictions=response, inputs = \" \", context = markdown_content\n",
    "        ).scores[0]\n",
    "\n",
    "        # Log metrics and pass model_id to link the metrics\n",
    "        mlflow.log_metrics(\n",
    "            {\n",
    "                f\"faithfulness_level_{level}\": faithfulness_score,\n",
    "            },\n",
    "            model_id=logged_model.model_id,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate with mlflow.evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlflow.entities import LoggedModelInput\n",
    "\n",
    "# Create evaluation dataset with different complexity levels\n",
    "levels = list(range(1, 6))  # Complexity levels 1-5\n",
    "eval_df = pd.DataFrame(\n",
    "    {\n",
    "        \"level\": levels,\n",
    "        \"text\": [markdown_content] * len(levels),  # Use the same text for each level\n",
    "    }\n",
    ")\n",
    "\n",
    "with mlflow.start_run(log_system_metrics=True) as evaluation_run:\n",
    "    # Generate predictions for each complexity level\n",
    "    predictions = []\n",
    "    for level in levels:\n",
    "        response = (\n",
    "            client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt.format(level=level, text=markdown_content)}\n",
    "                ],\n",
    "                model=\"gpt-4o-mini\",\n",
    "                temperature=1.5,\n",
    "                max_tokens=2000,\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message.content\n",
    "        )\n",
    "        predictions.append(response)\n",
    "\n",
    "    eval_df[\"predictions\"] = predictions\n",
    "\n",
    "    # Start a run to represent the evaluation job\n",
    "\n",
    "    eval_dataset = mlflow.data.from_pandas(\n",
    "        df=eval_df,\n",
    "        name=\"summarization_eval_dataset\",\n",
    "        targets=\"text\",  # Original text is our target\n",
    "        predictions=\"predictions\",  # Model summaries are our predictions\n",
    "    )\n",
    "    \n",
    "    mlflow.log_input(\n",
    "        dataset=eval_dataset, \n",
    "        model=LoggedModelInput(logged_model.model_id)\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Run the evaluation with faithfulness metric\n",
    "    result = mlflow.evaluate(\n",
    "        data=eval_dataset,\n",
    "        extra_metrics=[\n",
    "            mlflow.metrics.genai.faithfulness(\"openai:/gpt-4o\"),\n",
    "        ],\n",
    "        evaluator_config={\n",
    "            \"col_mapping\": {\n",
    "                \"inputs\": \"text\",  # Original text\n",
    "                \"context\": \"text\",  # Required for faithfulness metric\n",
    "            }\n",
    "        },\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
